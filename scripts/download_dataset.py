#!/usr/bin/env python3
"""
Download and prepare Tamil datasets for training.
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any
import json

from datasets import load_dataset
from tqdm.auto import tqdm
import pandas as pd

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def download_thamizhmalai(output_dir: str = "./data/Thamizhmalai") -> None:
    """Download Thamizhmalai dataset."""
    logger.info("Downloading Thamizhmalai dataset...")
    
    try:
        # Create output directory
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Load dataset
        dataset = load_dataset("tamiltheorist/Thamizhmalai")
        
        # Save to local files
        for split_name, split_data in dataset.items():
            output_file = Path(output_dir) / f"{split_name}.jsonl"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                for item in tqdm(split_data, desc=f"Saving {split_name}"):
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
            logger.info(f"Saved {split_name} split with {len(split_data)} samples to {output_file}")
        
        logger.info("тЬЕ Thamizhmalai dataset downloaded successfully!")
        
    except Exception as e:
        logger.error(f"тЭМ Error downloading Thamizhmalai: {e}")
        raise


def download_additional_datasets(output_dir: str = "./data") -> None:
    """Download additional Tamil datasets."""
    
    datasets_to_download = [
        {
            "name": "Tamil Wikipedia",
            "hf_name": "wikipedia",
            "config": "ta",
            "output_name": "tamil_wikipedia"
        },
        {
            "name": "Tamil News",
            "hf_name": "mlsum",
            "config": "ta",
            "output_name": "tamil_news"
        },
        # Add more datasets here
    ]
    
    for dataset_info in datasets_to_download:
        try:
            logger.info(f"Downloading {dataset_info['name']}...")
            
            # Create output directory
            dataset_dir = Path(output_dir) / dataset_info['output_name']
            dataset_dir.mkdir(parents=True, exist_ok=True)
            
            # Load dataset
            if dataset_info.get('config'):
                dataset = load_dataset(
                    dataset_info['hf_name'], 
                    dataset_info['config']
                )
            else:
                dataset = load_dataset(dataset_info['hf_name'])
            
            # Save dataset
            for split_name, split_data in dataset.items():
                output_file = dataset_dir / f"{split_name}.jsonl"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    for item in tqdm(split_data, desc=f"Saving {split_name}"):
                        f.write(json.dumps(item, ensure_ascii=False) + '\n')
                
                logger.info(f"Saved {dataset_info['name']} {split_name} to {output_file}")
            
        except Exception as e:
            logger.warning(f"тЪая╕П Could not download {dataset_info['name']}: {e}")
            continue


def prepare_instruction_data(data_dir: str = "./data") -> None:
    """Prepare instruction-following data from downloaded datasets."""
    logger.info("Preparing instruction-following data...")
    
    instruction_data = []
    
    # Process Thamizhmalai data
    thamizhmalai_dir = Path(data_dir) / "Thamizhmalai"
    if thamizhmalai_dir.exists():
        for file_path in thamizhmalai_dir.glob("*.jsonl"):
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        item = json.loads(line)
                        
                        # Convert to instruction format
                        if 'instruction' in item and 'output' in item:
                            instruction_data.append({
                                "instruction": item['instruction'],
                                "input": item.get('input', ''),
                                "output": item['output'],
                                "source": "Thamizhmalai"
                            })
                        elif 'question' in item and 'answer' in item:
                            instruction_data.append({
                                "instruction": item['question'],
                                "input": "",
                                "output": item['answer'],
                                "source": "Thamizhmalai"
                            })
                            
                    except json.JSONDecodeError:
                        continue
    
    # Add some basic Tamil conversation examples
    basic_conversations = [
        {
            "instruction": "ро╡рогроХрпНроХроорпН",
            "input": "",
            "output": "ро╡рогроХрпНроХроорпН! роиро╛ройрпН роТро░рпБ родрооро┐ро┤рпН AI роЙродро╡ро┐ропро╛ро│ро░рпН. роЙроЩрпНроХро│рпБроХрпНроХрпБ роОрокрпНрокроЯро┐ роЙродро╡ роорпБроЯро┐ропрпБроорпН?",
            "source": "manual"
        },
        {
            "instruction": "роирпАроЩрпНроХро│рпН роОрокрпНрокроЯро┐ роЗро░рпБроХрпНроХро┐ро▒рпАро░рпНроХро│рпН?",
            "input": "",
            "output": "роиро╛ройрпН роиройрпНро▒ро╛роХ роЗро░рпБроХрпНроХро┐ро▒рпЗройрпН. роиройрпНро▒ро┐ роХрпЗроЯрпНроЯродро▒рпНроХрпБ! роирпАроЩрпНроХро│рпН роОрокрпНрокроЯро┐ роЗро░рпБроХрпНроХро┐ро▒рпАро░рпНроХро│рпН?",
            "source": "manual"
        },
        {
            "instruction": "родрооро┐ро┤рпН роорпКро┤ро┐ рокро▒рпНро▒ро┐ роЪрпКро▓рпНро▓рпБроЩрпНроХро│рпН",
            "input": "",
            "output": "родрооро┐ро┤рпН роорпКро┤ро┐ роЙро▓роХро┐ройрпН рокро┤роорпИропро╛рой роорпКро┤ро┐роХро│ро┐ро▓рпН роТройрпНро▒рпБ. роЗродрпБ роЪроЩрпНроХ роЗро▓роХрпНроХро┐ропроорпН роорпБродро▓рпН роиро╡рпАрой роЗро▓роХрпНроХро┐ропроорпН ро╡ро░рпИ ро╡ро│рооро╛рой рокро╛ро░роорпНрокро░ро┐ропроорпН роХрпКрогрпНроЯродрпБ. родрооро┐ро┤рпН роорпКро┤ро┐ропро┐ройрпН роЗро▓роХрпНроХрогроорпН рооро┐роХро╡рпБроорпН роорпБро▒рпИропро╛ройродрпБ рооро▒рпНро▒рпБроорпН роЕро┤роХро╛ройродрпБ.",
            "source": "manual"
        },
        {
            "instruction": "роЙроЩрпНроХро│ро╛ро▓рпН роОройрпНрой роЪрпЖропрпНроп роорпБроЯро┐ропрпБроорпН?",
            "input": "",
            "output": "роиро╛ройрпН родрооро┐ро┤ро┐ро▓рпН роЙро░рпИропро╛роЯро▓рпН роироЯродрпНрод роорпБроЯро┐ропрпБроорпН, роХрпЗро│рпНро╡ро┐роХро│рпБроХрпНроХрпБ рокродро┐ро▓рпН роЕро│ро┐роХрпНроХ роорпБроЯро┐ропрпБроорпН, родрооро┐ро┤рпН роорпКро┤ро┐ рооро▒рпНро▒рпБроорпН роХро▓ро╛роЪрпНроЪро╛ро░роорпН рокро▒рпНро▒ро┐ ро╡ро┐ро│роХрпНроХ роорпБроЯро┐ропрпБроорпН, рооро▒рпНро▒рпБроорпН рокро▓рпНро╡рпЗро▒рпБ ро╡ро┐рд╖ропроЩрпНроХро│ро┐ро▓рпН роЙродро╡ро┐ роЪрпЖропрпНроп роорпБроЯро┐ропрпБроорпН.",
            "source": "manual"
        }
    ]
    
    instruction_data.extend(basic_conversations)
    
    # Save prepared data
    output_file = Path(data_dir) / "instruction_data.jsonl"
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in instruction_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    logger.info(f"тЬЕ Prepared {len(instruction_data)} instruction examples and saved to {output_file}")


def main():
    """Main function to download and prepare datasets."""
    logger.info("ЁЯЪА Starting dataset download and preparation...")
    
    # Create data directory
    os.makedirs("./data", exist_ok=True)
    
    # Download datasets
    download_thamizhmalai()
    download_additional_datasets()
    
    # Prepare instruction data
    prepare_instruction_data()
    
    logger.info("тЬЕ Dataset preparation completed!")


if __name__ == "__main__":
    main()
